{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------    Importing packages\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "#from torch.utils.data.sampler import SubsetRandomSampler\n",
    "#import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------      Data preprocessing\n",
    "is_cuda=False\n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "data_path = 'asl_alpha'\n",
    "\n",
    "aslDataset = torchvision.datasets.ImageFolder(\n",
    "    root=data_path,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# To find the classes\n",
    "aslDataset.classes\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "aslLoader = torch.utils.data.DataLoader(aslDataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Get some images from dataset\n",
    "dataiter = iter(aslLoader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if normalize, used the same means and std in the plotting.\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.title(labels.numpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([0.5, 0.5, 0.5])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(aslLoader))\n",
    "class_names = aslDataset.classes\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epoch, model, data_loader, phase='training', volatile=False):\n",
    "    if phase == 'training':\n",
    "        model.train()\n",
    "    if phase == 'validation':\n",
    "        model.eval()\n",
    "        volatile = True\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    \n",
    "    #label_accumulator = None\n",
    "    #output_accumulator =  np.zeros((len(data_loader),4))\n",
    "    #test_size_counter = 0\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        if is_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile), Variable(target)\n",
    "        ##print('data :', type(data), '****', 'target :', type(target)) ---- to delete\n",
    "        if phase == 'training':\n",
    "            optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        #loss = criterion(output, target)\n",
    "\n",
    "        running_loss += F.nll_loss(output, target, reduction='sum').data.item()\n",
    "        #running_loss += criterion(output, target).data[0]\n",
    "        preds = output.data.max(dim=1, keepdim=True)[1]\n",
    "        \n",
    "        #Saving values to compute AUC/ROC curve        \n",
    "        #label_accumulator.append(target)\n",
    "        #for j in range(data_loader.batch_size):\n",
    "        #    for i in range(4):\n",
    "        #         output_accumulator[test_size_counter][i] = output[j][i].cpu().detach().numpy()\n",
    "        #test_size_counter += 1\n",
    "        \n",
    "            \n",
    "        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        \n",
    "        if phase == 'training':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    loss = running_loss / len(data_loader.dataset)\n",
    "    accuracy = 100. * running_correct / len(data_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        '{} loss is {} and {} accuracy is {}/{}, {}'.format(phase, loss, phase, running_correct, len(data_loader.dataset), accuracy ))\n",
    "    return loss, accuracy, output, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "for epoch in range(1, 3):\n",
    "    epoch_loss, epoch_accuracy, output, labels = fit(epoch, cnn, train_loader, phase='training')\n",
    "    val_epoch_loss, val_epoch_accuracy, output, labels = fit(epoch, cnn, val_loader, phase='validation')\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # writer_train.add_scalars(\"loss training\", epoch_loss, epoch)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    # writer_train.add_scalars(\"losses\", {'train':epoch_loss, 'val':val_epoch_loss}, epoch) #OK\n",
    "    # writer_train.add_scalars(\"accuracies\", {'train':epoch_loss, 'val':val_epoch_loss}, epoch)\n",
    "\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "# Getting the whole examples and their prediction\n",
    "examples = np.zeros(((val_dataset.data[0].shape[0]),4))\n",
    "for i in range((val_dataset.data[0].shape[0])):\n",
    "    for j in range(4):\n",
    "        examples[i][j] = np.round(output[i][j].cpu().detach().numpy(),2)\n",
    "\n",
    "label_binarized = labels.cpu().detach().numpy()\n",
    "label_binarized = label_binarize(label_binarized,classes=[0,1,2,3])\n",
    "\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(4):\n",
    "    fpr[i], tpr[i], _ = roc_curve(label_binarized[:, i], examples[:,i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(label_binarized.ravel(), examples.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(fpr[3], tpr[3], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[3])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Cat classification performance')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Ship classification performance')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "#plt.savefig('ROC_best_worst.png')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(4)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(4):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= 4\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'navy', 'deeppink', 'green', 'yellow', 'black','red','darkblue'])\n",
    "plt.figure()\n",
    "for i, color in zip(range(4), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='{0} (area = {1:0.2f})'''.format(classes[i], roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('All classes')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "#plt.savefig('all_classes.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom tensorboardX import SummaryWriter\n",
    "#SummaryWriter encapsulates everything\n",
    "writer_train = SummaryWriter('runs6/model1')\n",
    "#writer_val = SummaryWriter('runs2/model1/val')\n",
    "'''writer_hist = SummaryWriter('runs1/model1/train')\n",
    "writer_model = SummaryWriter('runs1/model1/train')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
